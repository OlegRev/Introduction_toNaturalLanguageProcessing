{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkpcHsV8RWHA"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQBOJRARev7"
      },
      "source": [
        "**Написать теггер на данных с руским языком**\n",
        "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
        "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
        "3. сравнить все реализованные методы сделать выводы\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPRx8Cu_RDY1",
        "outputId": "6d683927-7bf5-4956-b929-4496b1d8442c"
      },
      "source": [
        "!pip install pyconll"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyconll in /usr/local/lib/python3.7/dist-packages (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgL-33mWUyZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyconll\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8htlG52iCa1"
      },
      "source": [
        "## загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXxwW9NzW570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9429e75-a229-4b5f-f08c-86d4078590f6"
      },
      "source": [
        "!mkdir datasets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpwgA3svWiRw",
        "outputId": "d529b414-22c2-4d18-8a1f-60e80d2f75d5"
      },
      "source": [
        "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-21 20:43:37--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81039282 (77M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-train.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  77.28M   166MB/s    in 0.5s    \n",
            "\n",
            "2021-09-21 20:43:38 (166 MB/s) - ‘./datasets/ru_syntagrus-ud-train.conllu’ saved [81039282/81039282]\n",
            "\n",
            "--2021-09-21 20:43:38--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10902738 (10M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  10.40M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-09-21 20:43:39 (93.3 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [10902738/10902738]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oymo30RBWjjl"
      },
      "source": [
        "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')\n",
        "df_accuracy  = pd.DataFrame(columns=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4Q4J9y0xzsQ"
      },
      "source": [
        "train_data = [[(token.form, token.upos) for token in sent] for sent in full_train]\n",
        "test_data = [[(token.form, token.upos) for token in sent] for sent in full_test]\n",
        "\n",
        "test_sent = [[form for form, _ in row] for row in test_data]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qGVBft8I1gr",
        "outputId": "e67ccb3b-fb9c-4fa3-c3b3-b6d43339be51"
      },
      "source": [
        "test_sent_choice = np.random.choice(test_sent)\n",
        "test_sent_choice"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Нужны', 'новые', 'станки', ',', 'нужны', 'новые', 'технологии', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiJDzY_wH80M",
        "outputId": "54e686f8-1a8f-4471-9986-544cd8185931"
      },
      "source": [
        "print(f'train_data: \\n{train_data[:2]}\\ntest_data:\\n{test_data[:2]}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data: \n",
            "[[('Анкета', 'NOUN'), ('.', 'PUNCT')], [('Начальник', 'NOUN'), ('областного', 'ADJ'), ('управления', 'NOUN'), ('связи', 'NOUN'), ('Семен', 'PROPN'), ('Еремеевич', 'PROPN'), ('был', 'AUX'), ('человек', 'NOUN'), ('простой', 'ADJ'), (',', 'PUNCT'), ('приходил', 'VERB'), ('на', 'ADP'), ('работу', 'NOUN'), ('всегда', 'ADV'), ('вовремя', 'ADV'), (',', 'PUNCT'), ('здоровался', 'VERB'), ('с', 'ADP'), ('секретаршей', 'NOUN'), ('за', 'ADP'), ('руку', 'NOUN'), ('и', 'CCONJ'), ('иногда', 'ADV'), ('даже', 'PART'), ('писал', 'VERB'), ('в', 'ADP'), ('стенгазету', 'NOUN'), ('заметки', 'NOUN'), ('под', 'ADP'), ('псевдонимом', 'NOUN'), ('\"', 'PUNCT'), ('Муха', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT')]]\n",
            "test_data:\n",
            "[[('Алгоритм', 'NOUN'), (',', 'PUNCT'), ('от', 'ADP'), ('имени', 'NOUN'), ('учёного', 'NOUN'), ('аль', 'PART'), ('-', 'PUNCT'), ('Хорезми', 'PROPN'), (',', 'PUNCT'), ('-', 'PUNCT'), ('точный', 'ADJ'), ('набор', 'NOUN'), ('инструкций', 'NOUN'), (',', 'PUNCT'), ('описывающих', 'VERB'), ('порядок', 'NOUN'), ('действий', 'NOUN'), ('исполнителя', 'NOUN'), ('для', 'ADP'), ('достижения', 'NOUN'), ('результата', 'NOUN'), ('решения', 'NOUN'), ('задачи', 'NOUN'), ('за', 'ADP'), ('конечное', 'ADJ'), ('время', 'NOUN'), ('.', 'PUNCT')], [('В', 'ADP'), ('старой', 'ADJ'), ('трактовке', 'NOUN'), ('вместо', 'ADP'), ('слова', 'NOUN'), ('\"', 'PUNCT'), ('порядок', 'NOUN'), ('\"', 'PUNCT'), ('использовалось', 'VERB'), ('слово', 'NOUN'), ('\"', 'PUNCT'), ('последовательность', 'NOUN'), ('\"', 'PUNCT'), (',', 'PUNCT'), ('но', 'CCONJ'), ('по', 'ADP'), ('мере', 'NOUN'), ('развития', 'NOUN'), ('параллельности', 'NOUN'), ('в', 'ADP'), ('работе', 'NOUN'), ('компьютеров', 'NOUN'), ('слово', 'NOUN'), ('\"', 'PUNCT'), ('последовательность', 'NOUN'), ('\"', 'PUNCT'), ('стали', 'VERB'), ('заменять', 'VERB'), ('более', 'ADV'), ('общим', 'ADJ'), ('словом', 'NOUN'), ('\"', 'PUNCT'), ('порядок', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk-SAMuMiHTf"
      },
      "source": [
        "## 1.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "E67kyZO-bFFE",
        "outputId": "6dd72480-fba2-4286-e40e-7fec806a5c09"
      },
      "source": [
        "taggers = [UnigramTagger, BigramTagger, TrigramTagger]\n",
        "\n",
        "for tag_c in taggers:\n",
        "  tagger = tag_c(train_data)\n",
        "  print(tag_c.__name__)\n",
        "  display(tagger.tag(test_sent_choice), tagger.evaluate(test_data))\n",
        "  df_accuracy.loc[tag_c.__name__, 'accuracy'] = tagger.evaluate(train_data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UnigramTagger\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Нужны', 'ADJ'),\n",
              " ('новые', 'ADJ'),\n",
              " ('станки', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('нужны', 'ADJ'),\n",
              " ('новые', 'ADJ'),\n",
              " ('технологии', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8772537323492737"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigramTagger\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Нужны', 'ADJ'),\n",
              " ('новые', 'ADJ'),\n",
              " ('станки', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('нужны', 'ADJ'),\n",
              " ('новые', 'ADJ'),\n",
              " ('технологии', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.6963064064974893"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrigramTagger\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Нужны', 'ADJ'),\n",
              " ('новые', None),\n",
              " ('станки', None),\n",
              " (',', None),\n",
              " ('нужны', None),\n",
              " ('новые', None),\n",
              " ('технологии', None),\n",
              " ('.', None)]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.24808748694099012"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC6KKJQHQOxo",
        "outputId": "3d73ec12-239f-4e91-dc36-e86315fde114"
      },
      "source": [
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "backoff = DefaultTagger('NOUN') \n",
        "tag = backoff_tagger(train_data,  \n",
        "                     taggers,  \n",
        "                     backoff = backoff) \n",
        "\n",
        "df_accuracy.loc['BackoffTagger', 'accuracy'] = tag.evaluate(test_data)\n",
        "tag.evaluate(test_data) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9119991237825633"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0XsZwbfiODw"
      },
      "source": [
        "## 1.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAIZCE9PCQNG",
        "outputId": "72256bc8-4a0a-459d-d1ae-7ec1dbe8fb0f"
      },
      "source": [
        "%%time\n",
        "train_tok = [form for row in train_data  for form, _ in row]\n",
        "train_label = ['NO_TAG' if upos is None else upos for row in train_data for _, upos in row]\n",
        "\n",
        "test_tok = [form for row in test_data for form, _ in row ]\n",
        "test_label = ['NO_TAG' if upos is None else upos for row in test_data for _, upos in row]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 343 ms, sys: 993 µs, total: 344 ms\n",
            "Wall time: 345 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZG-7sYfiQbi"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y-zKyEpe1K_b",
        "outputId": "af780668-ee36-477f-93ca-ca8bc0a39bb2"
      },
      "source": [
        "HashingVectorizer.__name__"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'HashingVectorizer'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDyo-qj2KZfP"
      },
      "source": [
        "def get_accuracy_vec(vectorizer, train_t, test_t, train_l, test_l):\n",
        "  \n",
        "  le = LabelEncoder()\n",
        "  train_enc_l = le.fit_transform(train_l)\n",
        "  test_enc_l = le.transform(test_l)\n",
        "\n",
        "  X_train = vectorizer.fit_transform(train_t)\n",
        "  X_test = vectorizer.transform(test_t)\n",
        "  \n",
        "  print(f'{X_train.shape}\\n')\n",
        "\n",
        "  lr = LogisticRegression(random_state=0)\n",
        "  lr.fit(X_train, train_enc_l)\n",
        "  pred = lr.predict(X_test)\n",
        "  # print(vectorizer.__name__)\n",
        "  print(vectorizer.get_params())\n",
        "  if str(vectorizer).split('(')[0] == 'HashingVectorizer':\n",
        "    vec_name = f\"{str(vectorizer).split('(')[0]}: [analyzer:{vectorizer.get_params()['analyzer']}, n_features:{vectorizer.get_params()['n_features']}]\"\n",
        "  else: vec_name = f\"{str(vectorizer).split('(')[0]}: [analyzer:{vectorizer.get_params()['analyzer']}]\"\n",
        "  df_accuracy.loc[vec_name, 'accuracy'] = accuracy_score(test_enc_l, pred)\n",
        "  return f'Accuracy: {accuracy_score(test_enc_l, pred)}'\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8FtaQnve3rK"
      },
      "source": [
        "vectorizers = [HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=100),\n",
        "               HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=1000),\n",
        "               HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=1000),\n",
        "               CountVectorizer(ngram_range=(1, 5), analyzer='char'),  #, binary=False, tokenizer=str.split),\n",
        "               TfidfVectorizer(ngram_range=(1, 5), analyzer='char'),  #, binary=False, tokenizer=str.split),\n",
        "               ]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VPZ8PlPE4n7",
        "outputId": "26f18793-f56c-4d5b-cdbb-098227f17251"
      },
      "source": [
        "for vectorizer in vectorizers:\n",
        "  print(get_accuracy_vec(vectorizer=vectorizer,\n",
        "                         train_t=train_tok,\n",
        "                         test_t=test_tok,\n",
        "                         train_l=train_label,\n",
        "                         test_l=test_label))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(871526, 100)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'alternate_sign': True, 'analyzer': 'char', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'n_features': 100, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None}\n",
            "Accuracy: 0.686221480807468\n",
            "(871526, 1000)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'alternate_sign': True, 'analyzer': 'char', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'n_features': 1000, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None}\n",
            "Accuracy: 0.8805058470663566\n",
            "(871526, 1000)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'alternate_sign': True, 'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'n_features': 1000, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None}\n",
            "Accuracy: 0.3789303407137802\n",
            "(871526, 149809)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'analyzer': 'char', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n",
            "Accuracy: 0.949979779597614\n",
            "(871526, 149809)\n",
            "\n",
            "{'analyzer': 'char', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 5), 'norm': 'l2', 'preprocessor': None, 'smooth_idf': True, 'stop_words': None, 'strip_accents': None, 'sublinear_tf': False, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'use_idf': True, 'vocabulary': None}\n",
            "Accuracy: 0.9382266707107472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCMyGcARiRW8"
      },
      "source": [
        "## 1.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "rcdF-oBfhHl2",
        "outputId": "a88005bc-38ab-4e82-ab2c-79ba0398d038"
      },
      "source": [
        "df_accuracy"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>UnigramTagger</th>\n",
              "      <td>0.974003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BigramTagger</th>\n",
              "      <td>0.970001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TrigramTagger</th>\n",
              "      <td>0.894157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BackoffTagger</th>\n",
              "      <td>0.911999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HashingVectorizer: [analyzer:char, n_features:100]</th>\n",
              "      <td>0.686221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HashingVectorizer: [analyzer:char, n_features:1000]</th>\n",
              "      <td>0.880506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HashingVectorizer: [analyzer:word, n_features:1000]</th>\n",
              "      <td>0.37893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CountVectorizer: [analyzer:char]</th>\n",
              "      <td>0.94998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TfidfVectorizer: [analyzer:char]</th>\n",
              "      <td>0.938227</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    accuracy\n",
              "UnigramTagger                                       0.974003\n",
              "BigramTagger                                        0.970001\n",
              "TrigramTagger                                       0.894157\n",
              "BackoffTagger                                       0.911999\n",
              "HashingVectorizer: [analyzer:char, n_features:100]  0.686221\n",
              "HashingVectorizer: [analyzer:char, n_features:1...  0.880506\n",
              "HashingVectorizer: [analyzer:word, n_features:1...   0.37893\n",
              "CountVectorizer: [analyzer:char]                     0.94998\n",
              "TfidfVectorizer: [analyzer:char]                    0.938227"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehmbYXMgBA9P"
      },
      "source": [
        "Прирос при равных параметрах у HashingVect при увеличении количеста фичей.\n",
        "При внесении тех же данных при смене анализатора падение точности.\n",
        "\n",
        "Небольшой выигрыш CountVectorizer у TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cINqgGpKXURp"
      },
      "source": [
        "# Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCM0drjKXYet"
      },
      "source": [
        "много дополнительных датасетов на русском языке\n",
        "\n",
        "https://natasha.github.io/corus/  \n",
        "https://github.com/natasha/corus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUOg4C8sZNpw"
      },
      "source": [
        "мы будем использовать данные http://www.labinform.ru/pub/named_entities/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzi6ApNLZg6X"
      },
      "source": [
        "**Проверить насколько хорошо работает NER**\n",
        "\n",
        "1. взять нер из nltk\n",
        "2. проверить deeppavlov\n",
        "3. написать свой нер попробовать разные подходы:\n",
        "* передаём в сетку токен и его соседей\n",
        "* передаём в сетку только токен\n",
        "\n",
        "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1LgaNUtaOz"
      },
      "source": [
        "при обучении своего нера незабудьте разделить выборку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg6tcss2Zhp9"
      },
      "source": [
        "!pip install corus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrc5ocDkaS1e"
      },
      "source": [
        "import corus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPVv6lhT3ftA"
      },
      "source": [
        "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtLK4z9CH2Ph"
      },
      "source": [
        "!unzip collection5.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPxCa2LyH8gW"
      },
      "source": [
        "!rm collection5.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7burz-bD3t4l"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEdS2pAS3fod"
      },
      "source": [
        "from corus import load_ne5\n",
        "\n",
        "dir = 'Collection5/'\n",
        "records = load_ne5(dir)\n",
        "next(records)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrhLNgNwQP2P"
      },
      "source": [
        "процедуры обработки взять из вебинарного ноутбука"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRuODJpkIqlv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDdnL6EXJRt9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skYaNCiC5xM4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kd-emBao1u-"
      },
      "source": [
        "# установка deeppavlov\n",
        "\n",
        "!pip uninstall -y tensorflow tensorflow-gpu\n",
        "!pip install numpy scipy librosa unidecode inflect librosa transformers\n",
        "!pip install deeppavlov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY96lqBzsZJ_"
      },
      "source": [
        "#!python -m deeppavlov install squad_bert\n",
        "#!python -m deeppavlov install ner_ontonotes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KE7tpVWs1b7"
      },
      "source": [
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsegbgCbrzy_"
      },
      "source": [
        "deeppavlov_ner = build_model(configs.ner.ner_bert_ent_and_type_rus, download=True)\n",
        "rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n",
        "deeppavlov_ner([rus_document])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ1phPKisJMz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}